{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$\\Large\\textbf{Answer 2}$"
      ],
      "metadata": {
        "id": "x0qYHYq1OAHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z-p9YanbednX"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary packages\n",
        "import numpy as np \n",
        "from numpy.linalg import cond\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "import pandas as pd\n",
        "from scipy.linalg import sqrtm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method to find Hessian matrix\n",
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return (400*x[0]**2 + 0.004*x[1]**4)\n",
        "     \n",
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.array([800*x[0],0.016*x[1]**3])\n",
        "\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2\n",
        "  return np.array([[800,0],[0,0.048*x[1]**2]])\n",
        "\n",
        "#method to find the condition number of any square matrix\n",
        "#def find_condition_number(A):\n",
        " # assert type(A) is np.ndarray\n",
        "  #assert A.shape[0] == A.shape[1]\n",
        "  #return cond(A)"
      ],
      "metadata": {
        "id": "s6Ke98FJiNaD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The method defines a way to construct D_k matrix used in scaling the gradient in the modified gradient descent scheme\n",
        "\n",
        "def compute_D_k(x):\n",
        "    assert type(x) is np.ndarray\n",
        "    assert len(x) == 2\n",
        "    \n",
        "    elem1 = evalh(x)[0][0]\n",
        "    elem2 = evalh(x)[1][1]\n",
        "\n",
        "    return np.diag([1/elem1,1/elem2])\n",
        "\n",
        "def compute_D_k_newton(x):\n",
        "    assert type(x) is np.ndarray\n",
        "    assert len(x) == 2\n",
        "    \n",
        "    return np.linalg.inv(evalh(x))\n",
        "  "
      ],
      "metadata": {
        "id": "njlqy1GCiNkG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Module to compute the steplength by using the closed-form expression\n",
        "\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "    assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "    assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "    #assert type(alpha_start) is float and alpha_start>=0. \n",
        "    assert type(rho) is float and rho>=0.\n",
        "    assert type(gamma) is float and gamma>=0. \n",
        " \n",
        "    alpha = alpha_start\n",
        "    p = -gradf\n",
        "    while evalf(np.add(x, alpha*p)) > np.add(evalf(x), gamma*alpha*np.dot(gradf.T,p)):\n",
        "        alpha = rho*alpha\n",
        "\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "sK7JsmABiNnW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, D_k, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "    assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "    assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "    #assert type(alpha_start) is float and alpha_start>=0. \n",
        "    assert type(rho) is float and rho>=0.\n",
        "    assert type(gamma) is float and gamma>=0. \n",
        "    assert type(D_k) is np.ndarray and len(D_k) == 2\n",
        "    \n",
        "    alpha = alpha_start\n",
        "    p = -gradf\n",
        "    while evalf(np.add(x, alpha*np.dot(D_k,p))) > np.subtract(evalf(x), gamma*alpha*np.dot(np.dot(D_k,gradf), gradf)):\n",
        "    \n",
        "        alpha = rho*alpha\n",
        "    \n",
        "    return alpha "
      ],
      "metadata": {
        "id": "wkfY8UpBiNp6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# line search type \n",
        "CONSTANT_STEP_LENGTH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "\n",
        "# D_k choice type\n",
        "newton = 3\n",
        "diagonal_approx = 4"
      ],
      "metadata": {
        "id": "Dbom4MTniNtD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for gradient descent with scaling to find the minimizer without scaling\n",
        "\n",
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\n",
        "    #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "    assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "    assert type(tol) is float and tol>=0 \n",
        "\n",
        "    x = start_x\n",
        "    g_x = evalg(x)\n",
        "\n",
        "    #initialization for backtracking line search\n",
        "    if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "        alpha_start = args[0]\n",
        "        rho = args[1]\n",
        "        gamma = args[2]\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    while (np.linalg.norm(g_x) > tol): \n",
        "    \n",
        "        if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "            step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) \n",
        "        elif line_search_type == CONSTANT_STEP_LENGTH: \n",
        "            step_length = 1.0\n",
        "        else:  \n",
        "            raise ValueError('Line search type unknown. Please check!')\n",
        "        \n",
        "        # Gradient descent steps   \n",
        "        x = np.subtract(x, np.multiply(step_length,g_x))\n",
        "        k += 1 \n",
        "        g_x = evalg(x) \n",
        "\n",
        "        #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "    return x, k, evalf(x) \n",
        " "
      ],
      "metadata": {
        "id": "X9CpLOY3iNwa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for gradient descent with scaling to find the minimizer with scaling\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type, D_k_type, *args):\n",
        "    #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "    assert type(start_x) is np.ndarray and len(start_x) == 2 \n",
        "    assert type(tol) is float and tol>=0 \n",
        "\n",
        "    x = start_x\n",
        "    g_x = evalg(x)\n",
        "    \n",
        "    #initialization for backtracking line search\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    while (np.linalg.norm(g_x) > tol): \n",
        "        if D_k_type == newton:\n",
        "            d_k = compute_D_k_newton(x)\n",
        "        elif D_k_type == diagonal_approx:\n",
        "            d_k = compute_D_k(x)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid argument.\")\n",
        "\n",
        "\n",
        "        if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "            step_length = compute_steplength_backtracking_scaled_direction(x, g_x, d_k, alpha_start, rho, gamma) \n",
        "        elif line_search_type == CONSTANT_STEP_LENGTH: \n",
        "            step_length = 1.0\n",
        "        else:  \n",
        "            raise ValueError('Line search type unknown. Please check!')\n",
        "        \n",
        "        # Gradient descent steps\n",
        "        x = np.subtract(x, np.multiply(step_length,np.dot(d_k, g_x))) \n",
        "        k += 1 \n",
        "        g_x = evalg(x) \n",
        "        #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "\n",
        "    return x, k, evalf(x)\n",
        " "
      ],
      "metadata": {
        "id": "mA9CNv8MjaF3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\Large\\textbf{Answer 3}$"
      ],
      "metadata": {
        "id": "IkzmLfhAOIWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.0,2.0])\n",
        "my_tol= 1e-9\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ],
      "metadata": {
        "id": "QRfVPP2Vj4m4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent using Newton's method with constant step length with scaling\n",
        "print('Results for constant step length')\n",
        "minimizer, iterations, objective = find_minimizer_gdscaling(my_start_x, my_tol, CONSTANT_STEP_LENGTH, newton, alpha_start, rho, gamma)\n",
        "print('Minimizer:', minimizer, '\\nObjective function value:', objective, '\\nIterations taken:', iterations)\n",
        "\n",
        "\n",
        "# Gradient descent using Newton's method with backtracking line search with scaling\n",
        "print('Results for backtracking line search')\n",
        "minimizer, iterations, objective = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, newton, alpha_start, rho, gamma)\n",
        "print('Minimizer:', minimizer, '\\nObjective function value:', objective, '\\nIterations taken:', iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3MLWlZ_j4qa",
        "outputId": "d10a83e9-8982-435c-d9d0-28d3bf397f2c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for constant step length\n",
            "Minimizer: [0.         0.00304488] \n",
            "Objective function value: 3.4382653805813626e-13 \n",
            "Iterations taken: 16\n",
            "Results for backtracking line search\n",
            "Minimizer: [0.         0.00304488] \n",
            "Objective function value: 3.4382653805813626e-13 \n",
            "Iterations taken: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\Large\\textbf{Observations}$"
      ],
      "metadata": {
        "id": "TtlFzH8eOjI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimizer:**  [0.         0.00304488] \n",
        "\n",
        "**Objective function value:**  3.4382653805813626e-13 \n",
        "\n",
        "**Iterations taken:** 16\n",
        "\n",
        "*  The above result is same for the both the cases i.e., in newton's method with constant step length and in newton's method with backtracking line search.\n",
        "\n",
        "*  We can see that the choice of steplength did not affect the convergence of algorithm."
      ],
      "metadata": {
        "id": "DajR8CN4OwUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the code for few hours the algorithm stops due to very low tolerance so we'll choose greater tolerance say 10^-6."
      ],
      "metadata": {
        "id": "gYmLx05dIQ01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.0,2.0])\n",
        "my_tol= 1e-6\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ],
      "metadata": {
        "id": "4Cptu4ZrIsam"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent with backtracking line search without scaling\n",
        "minimizer, iters, objective = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho, gamma)\n",
        "print('Minimizer:', minimizer, '\\nFinal Objective function value:', objective, '\\nIterations taken to terminate:', iters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgD13GstI5T4",
        "outputId": "8c493a47-e530-4ab0-d7b4-2dd1f9333ffb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer: [3.53148711e-10 3.91377499e-02] \n",
            "Final Objective function value: 9.385197325158433e-09 \n",
            "Iterations taken to terminate: 5707240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent using backtracking line search with scaling using diagonal matrix\n",
        "minimizer, iter, objective = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, diagonal_approx, alpha_start, rho, gamma)\n",
        "print('Minimizer:', minimizer ,'\\nFinal Objective function value:', objective, '\\nIterations taken to terminate:', iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOzmWEaRIylH",
        "outputId": "f3a63451-4536-41d7-e20c-e645103eeadc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer: [0.         0.03468306] \n",
            "Final Objective function value: 5.788014517642641e-09 \n",
            "Iterations taken to terminate: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\Large\\textbf{Observations}$"
      ],
      "metadata": {
        "id": "E37DMJt3UK7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For backtracking without scaling:**\n",
        "\n",
        "Minimizer: [3.53148711e-10 3.91377499e-02] \n",
        "\n",
        "Final Objective function value: 9.385197325158433e-09 \n",
        "\n",
        "Iterations taken to terminate: 5707240\n",
        "\n",
        "**For backtracking with scaling using diagonal matrix:**\n",
        "\n",
        "Minimizer: [0.         0.03468306] \n",
        "\n",
        "Final Objective function value: 5.788014517642641e-09 \n",
        "\n",
        "Iterations taken to terminate: 10\n",
        "\n",
        "When we took lesser tolerance value (1e-9) it took hours long to converge without scaling. But for 1e-6, it took only 10 minutes.\n",
        "\n",
        "The number of iterations taken by with scaling is is much much lesser than taken by without scaling."
      ],
      "metadata": {
        "id": "a3mMKzosUSAf"
      }
    }
  ]
}